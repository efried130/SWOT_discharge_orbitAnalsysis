{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1a7315b-1370-416d-81c8-95f5900523dc",
   "metadata": {},
   "source": [
    "# Orbit Bias Analysis\n",
    "\n",
    "### Compare mean/median discharge from FS vs SO to check for systematic differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4fd6b3-0137-4458-8ad9-901fbe10002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge FS reaches into its own list\n",
    "\n",
    "# Credit to Nikki Tebaldi, https:/podaac.github.io/tutorials/notebooks/datasets/SWOT_L4_DAWG_SOS_DISCHARGE.html#plot-discharge-timeseries\n",
    "import datetime\n",
    "import time\n",
    "import pathlib\n",
    "import os,sys\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cartopy\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import folium\n",
    "import requests\n",
    "from io import StringIO\n",
    "import h5py\n",
    "import math\n",
    "import zipfile\n",
    "from datetime import timedelta\n",
    "import json\n",
    "import seaborn as sns \n",
    "import warnings\n",
    "from tqdm import tqdm \n",
    "from itertools import islice\n",
    "from scipy.stats import pearsonr\n",
    "from itertools import combinations\n",
    "from scipy.stats import genextreme\n",
    "from scipy.stats import ttest_1samp\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy import stats\n",
    "from scipy.stats import median_abs_deviation, shapiro, ttest_1samp, wilcoxon\n",
    "from scipy.stats import ks_2samp, wasserstein_distance, shapiro, wilcoxon\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Specialized Functions\n",
    "from swotOrbitFunctions_ungauged import *\n",
    "divide_date = pd.to_datetime('2023-07-11')\n",
    "\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 30,    # Title font size\n",
    "    'axes.labelsize': 22,     # Axis labels font size\n",
    "    'xtick.labelsize': 20,    # X-axis ticks font size\n",
    "    'ytick.labelsize': 20,    # Y-axis ticks font size\n",
    "    'legend.fontsize': 20,    # Legend font size\n",
    "    'font.size': 20,          # Global font size for text\n",
    "    'figure.titlesize': 24,   # Figure title font size\n",
    "    'lines.linewidth': 2,     # Line width\n",
    "    'axes.linewidth': 2,      # Axis line width\n",
    "    'axes.grid': True,        # Show grid\n",
    "    'grid.linestyle': '--',   # Dashed grid lines\n",
    "    'grid.alpha': 0.5,        # Grid line transparency\n",
    "    'figure.figsize': (10, 6) # Figure size (width, height in inches)\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "def get_season_orbits(date):\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    if (month == 3 and day >= 30) or (month in [4, 5, 6]) or (month == 7 and day < 12):\n",
    "        return '4-7'\n",
    "    elif (month == 7 and day >= 12) or (month in [7, 8]) or (month == 9 and day < 30):\n",
    "        return '7-10'\n",
    "    elif (month == 9 and day >= 30) or (month in [10, 11]):\n",
    "        return '10-1'\n",
    "    else:\n",
    "        return '1-4'\n",
    "\n",
    "color_dict = {\n",
    "    'sic4dvar': 'green',\n",
    "    'momma': 'blue',\n",
    "    'neobam': 'purple',\n",
    "    'consensus': 'sienna',\n",
    "    'metroman': 'orange',\n",
    "    'geobam': 'purple',\n",
    "    'hivdi': 'deeppink',\n",
    "    'sad': 'tomato',\n",
    "    'gauge': 'black'\n",
    "}\n",
    "\n",
    "def tukey_filter(df, column, threshold=1.5):\n",
    "    \"\"\"\n",
    "    Apply Tukey outlier filtering to a specific column in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - column: column name (string) to apply Tukey filter on\n",
    "    - threshold: float, the multiplier for the IQR (default is 1.5)\n",
    "    \n",
    "    Returns:\n",
    "    - Filtered DataFrame where values are within [Q1 - threshold*IQR, Q3 + threshold*IQR]\n",
    "    \"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    filtered_df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    return filtered_df\n",
    "\n",
    "def calc_cons(df):\n",
    "    df = df[df['time_str'] != 'no_data']\n",
    "    if 'consensus' not in df['algo'].unique():\n",
    "            algo_Q_cons_values = (\n",
    "                df[df['algo'] != 'gauge']\n",
    "                .groupby(['reach_id', 'time'])['Q']\n",
    "                .median()\n",
    "                .reset_index()\n",
    "            )\n",
    "            algo_Q_cons_values['algo'] = 'consensus'\n",
    "            df = pd.concat([df, algo_Q_cons_values], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def modified_z_filter(series, threshold=3.5):\n",
    "    median = np.median(series)\n",
    "    mad = median_abs_deviation(series, scale='normal')\n",
    "    if mad == 0:\n",
    "        return series\n",
    "    mzs = 0.6745 * (series - median) / mad\n",
    "    return series[np.abs(mzs) <= threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c512ab71-5cf2-484f-a9ad-3a6c706ba7db",
   "metadata": {},
   "source": [
    "## Load Reach Discharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db3841-e557-49bc-a82b-31b14fcb264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reach hydrograph comparison\n",
    "\n",
    "runC_perm = pd.read_csv('/ContinuousRun').drop_duplicates(subset=['reach_id','algo','time_str'])\n",
    "\n",
    "runB_perm = pd.read_csv('/ScienceRun').drop_duplicates(subset=['reach_id','algo','time_str']) #runB_124_perm,  ,runC_6789_perm\n",
    "\n",
    "runA_perm = pd.read_csv('/FastRun').drop_duplicates(subset=['reach_id','algo','time_str']) #runB_124_perm,  ,runC_6789_perm\n",
    "\n",
    "runE_perm = pd.read_csv('/SampledRun').drop_duplicates(subset=['reach_id','algo','time_str']).dropna(subset='time_str')\n",
    "\n",
    "print(runA_perm.groupby(['algo']).reach_id.nunique())\n",
    "print(runB_perm.groupby(['algo']).reach_id.nunique())\n",
    "print(runC_perm.groupby(['algo']).reach_id.nunique())\n",
    "print(runE_perm.groupby(['algo']).reach_id.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4686b37-8eda-4751-ba9c-ffd5b19cbda4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd44729-1771-4e89-abf7-63be86df60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consensus and other Cleaning\n",
    "\n",
    "# Ensure 'time' column is in datetime format\n",
    "runC_perm['time'] = pd.to_datetime(runC_perm['time'])\n",
    "runC_perm = runC_perm[(runC_perm['Q'] > 1) & (runC_perm['Q'] < 1e7)]\n",
    "\n",
    "# Define the cutoff date\n",
    "divide_date = pd.Timestamp(\"2023-07-11\")\n",
    "\n",
    "# First, filter for 'algo' == 'consensus'\n",
    "runC_perm_consensus = runC_perm[runC_perm['algo'] == 'consensus']\n",
    "\n",
    "# Identify reach_ids that have at least 10 observations before and after the cutoff\n",
    "valid_reach_ids = runC_perm_consensus.groupby('reach_id').filter(\n",
    "    lambda g: (g['time'] < divide_date).sum() >= 10 and (g['time'] >= divide_date).sum() >= 10\n",
    ")['reach_id'].unique()\n",
    "\n",
    "# Apply this filter to the full dataset (all algorithms included)\n",
    "runC_perm_clean = runC_perm[runC_perm['reach_id'].isin(valid_reach_ids)]\n",
    "\n",
    "# Print the number of unique reaches\n",
    "print('# of unique reaches runC with min 10 values: ', runC_perm_clean['reach_id'].nunique())\n",
    "\n",
    "\n",
    "valid_reach_ids_C_min = runC_perm_consensus.groupby('reach_id').filter(\n",
    "    lambda g: (g['time'] < divide_date).sum() >= 1 and (g['time'] >= divide_date).sum() >= 1\n",
    ")['reach_id'].unique()\n",
    "\n",
    "# Apply this filter to the full dataset (all algorithms included)\n",
    "runC_perm_clean_min = runC_perm[runC_perm['reach_id'].isin(valid_reach_ids_C_min)]\n",
    "\n",
    "# Print the number of unique reaches\n",
    "print('# of unique reaches runC MIN 1 IN FS: ', runC_perm_clean_min['reach_id'].nunique())\n",
    "\n",
    "#RunA\n",
    "runA_perm['time'] = pd.to_datetime(runA_perm['time'])\n",
    "runA_perm = runA_perm[(runA_perm['Q'] > 1) & (runA_perm['Q'] < 1e7)]\n",
    "\n",
    "runA_perm_consensus = runA_perm[runA_perm['algo'] == 'consensus']\n",
    "\n",
    "valid_reach_ids_A = runA_perm_consensus.groupby('reach_id').filter(\n",
    "    lambda g: (g['time'] <= divide_date).sum() >= 10\n",
    ")['reach_id'].unique()\n",
    "runA_perm_clean = runA_perm[runA_perm['reach_id'].isin(valid_reach_ids_A)]\n",
    "print('# of unique reaches A with min 10 values: ', runA_perm_clean['reach_id'].nunique())\n",
    "\n",
    "#RUN B\n",
    "runB_perm['time'] = pd.to_datetime(runB_perm['time'])\n",
    "runB_perm = runB_perm[(runB_perm['Q'] > 1) & (runB_perm['Q'] < 1e7)]\n",
    "\n",
    "runB_perm_consensus = runB_perm[runB_perm['algo'] == 'consensus']\n",
    "\n",
    "valid_reach_ids_B = runB_perm_consensus.groupby('reach_id').filter(\n",
    "    lambda g: (g['time'] >= divide_date).sum() >= 10\n",
    ")['reach_id'].unique()\n",
    "\n",
    "runB_perm_clean = runB_perm[runB_perm['reach_id'].isin(valid_reach_ids_B)]\n",
    "runB_perm_clean = runB_perm_clean[runB_perm_clean['reach_id'].isin(valid_reach_ids_A)]\n",
    "\n",
    "print('# of unique reaches runB with min 10 values: ', runB_perm_clean['reach_id'].nunique())\n",
    "\n",
    "\n",
    "#RunD \n",
    "runE_perm = runE_perm.dropna(subset=['time'])\n",
    "\n",
    "runE_perm['time'] = pd.to_datetime(runE_perm['time'])\n",
    "runE_perm = runE_perm[(runE_perm['Q'] > 1) & (runE_perm['Q'] < 1e7)]\n",
    "\n",
    "runE_perm_consensus = runE_perm[runE_perm['algo'] == 'consensus']\n",
    "valid_reach_ids_D = runE_perm_consensus.groupby('reach_id').filter(\n",
    "    lambda g: (g['time'] <= divide_date).sum() >= 5\n",
    ")['reach_id'].unique()\n",
    "\n",
    "runE_perm_clean = runE_perm[runE_perm['reach_id'].isin(valid_reach_ids_D)]\n",
    "print('# of unique reaches E: ', runE_perm_clean['reach_id'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862183d6-928b-42cb-8008-457993d7b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "runC_perm_clean_cont = runC_perm_clean.copy()\n",
    "runC_perm_clean_cont['algo'] = runC_perm_clean_cont['algo'].replace({'geobam': 'neobam'})\n",
    "\n",
    "continent_map = {\n",
    "    '1': 'AF', '2': 'EU', '3': 'SI', '4': 'AS',\n",
    "    '5': 'OC', '6': 'SA', '7': 'NA', '8': 'AR', '9': 'GR'\n",
    "}\n",
    "\n",
    "runC_perm_clean_cont['continent'] = runC_perm_clean_cont['reach_id'].astype(str).str[0]\n",
    "runC_perm_clean_cont['continent_name'] = runC_perm_clean_cont['continent'].map(continent_map)\n",
    "\n",
    "\n",
    "# Step 2: Group by algo and continent, then count unique reach_ids\n",
    "result = runC_perm_clean_cont.groupby(['algo', 'continent_name'])['reach_id'].nunique().reset_index()\n",
    "\n",
    "\n",
    "# Pivot the data for plotting (so continent_name is index, algos as columns)\n",
    "pivot_df = result.pivot(index='continent_name', columns='algo', values='reach_id').fillna(0)\n",
    "\n",
    "# Plotting\n",
    "ax = pivot_df.plot(kind='bar', figsize=(10, 6), color=[color_dict.get(algo, '#333333') for algo in pivot_df.columns])\n",
    "\n",
    "#plt.title('Unique reach_id Counts per Continent by Algo')\n",
    "plt.xlabel('Continent')\n",
    "plt.ylabel('Frequency (Unique reach_id)')\n",
    "plt.xticks(rotation=45)\n",
    "ax.legend(title='Algorithm', bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fd544e-e46d-41eb-bc97-3f54f4c35d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset to common reach_ids\n",
    "\n",
    "def subset_to_common_reach_ids(dfs, reach_id_col='reach_id', algo_col='algo', consensus_value='consensus'):\n",
    "    \"\"\"\n",
    "    Finds common reach_ids across all DataFrames in the list where the algo column equals 'consensus',\n",
    "    and subsets each DataFrame to only those reach_ids.\n",
    "\n",
    "    Parameters:\n",
    "        dfs (list of pd.DataFrame): List of DataFrames to compare and subset.\n",
    "        reach_id_col (str): Name of the column that contains reach IDs.\n",
    "        algo_col (str): Name of the algorithm-identifying column.\n",
    "        consensus_value (str): The value in algo_col that marks consensus rows.\n",
    "\n",
    "    Returns:\n",
    "        list of pd.DataFrame: Subsetted DataFrames with only common consensus reach_ids.\n",
    "    \"\"\"\n",
    "    # Step 1: Find the set of common consensus reach_ids\n",
    "    common_ids = set(\n",
    "        dfs[0].loc[dfs[0][algo_col] == consensus_value, reach_id_col]\n",
    "    )\n",
    "    for df in dfs[1:]:\n",
    "        consensus_ids = set(df.loc[df[algo_col] == consensus_value, reach_id_col])\n",
    "        common_ids &= consensus_ids\n",
    "\n",
    "    # Step 2: Subset each DataFrame to those reach_ids\n",
    "    dfs_subset = [df[df[reach_id_col].isin(common_ids)].copy() for df in dfs]\n",
    "    #Save common reach ids\n",
    "    with open('common_reaches_abcd.json', 'w') as f:\n",
    "        json.dump(list(common_ids), f, indent=2)\n",
    "    \n",
    "    return dfs_subset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f7fd86-a17c-4366-a89f-149bf2254e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile for comparison analysis, all function input a dictionary of runs\n",
    "\n",
    "df_list = [runA_perm_clean, runB_perm_clean, runC_perm_clean] #, runE_perm_clean] #, runD_perm_clean] \n",
    "df_list_common = subset_to_common_reach_ids(df_list)\n",
    "print(df_list_common[0].reach_id.nunique())\n",
    "\n",
    "\n",
    "dfs_q = {'Fast': df_list_common[0], 'Science': df_list_common[1], 'Continuous': df_list_common[2], 'Sampled': runE_perm_clean}\n",
    "\n",
    "for label, df in dfs_q.items():\n",
    "    df['algo'] = df['algo'].replace({'geobam': 'neobam'})\n",
    "    print(label, df.reach_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fec812e-82ff-4a75-803c-f849ee882b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calc CV and RMD\n",
    "\n",
    "dfs_q =append_RMD(dfs_q=dfs_q)\n",
    "dfs_q = append_coeffVar(dfs_q=dfs_q)\n",
    "for label, df in dfs_q.items():\n",
    "    print(label, df.reach_id.nunique())\n",
    "# cdfPlot_RMD(dfs_q=dfs_q, color_dict=color_dict, column_to_plot = 'RMD_cons')\n",
    "# #cdfPlot_RMD(dfs_q=dfs_q, color_dict=color_dict, column_to_plot = 'RMD_gauge')\n",
    "# plot_cdf_coeff(dfs_q=dfs_q, color_dict=color_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a6e158-9184-4dfd-9c44-d85b9e1dcc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Count how many algos each reach_id has\n",
    "# algo_counts_per_reach = dfs_q['Continuous'].groupby('reach_id')['algo'].nunique()\n",
    "\n",
    "# Count frequency per algorithm\n",
    "algo_counts = dfs_q['Continuous'].groupby('algo')['reach_id'].nunique()\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "bars = plt.bar(algo_counts.index, algo_counts.values, color=[color_dict[a] for a in algo_counts.index])\n",
    "\n",
    "# Add counts on top of bars\n",
    "for bar in bars:\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2, \n",
    "        bar.get_height(), \n",
    "        str(int(bar.get_height())), \n",
    "        ha='center', va='bottom'\n",
    "    )\n",
    "\n",
    "plt.ylim([0, 12500])\n",
    "plt.ylabel('Number of Reaches')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.title('Algorithm Frequency (Continuous run)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7209d879-66ce-47db-a865-cdc13cd33733",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_q = remove_low_cv_and_recalc_consensus(dfs_q, CV_thresh = 0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4bfc4-f9f9-4e34-9835-2af0599005de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, df in dfs_q.items():\n",
    "        df = df.drop(columns=['CV', 'CV_cons', 'CV_gauge','RMD_cons'])\n",
    "        dfs_q[label] = df  # Save the modified DataFrame back\n",
    "dfs_q =append_RMD(dfs_q=dfs_q)\n",
    "dfs_q = append_coeffVar(dfs_q=dfs_q)\n",
    "cdfPlot_RMD(dfs_q=dfs_q, color_dict=color_dict, column_to_plot = 'RMD_cons')\n",
    "#cdfPlot_RMD(dfs_q=dfs_q, color_dict=color_dict, column_to_plot = 'RMD_gauge')\n",
    "plot_cdf_coeff(dfs_q=dfs_q, color_dict=color_dict, algos_to_plot=['sic4dvar', 'momma', 'hivdi', 'neobam', 'geobam', 'consensus', 'sad', 'metroman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fbe266-6c7b-4bbb-87f5-566461f061f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE FILTERED Q\n",
    "for label, df in dfs_q.items():\n",
    "    print(label, df.reach_id.nunique())\n",
    "\n",
    "#SAVES AND CALLS DEPENDING...\n",
    "# pd.DataFrame(dfs_q['Fast']).to_csv('/all_q_a_perm_filtered.csv', index=False)\n",
    "# pd.DataFrame(dfs_q['Science']).to_csv('/all_q_b_perm_filtered.csv', index=False)\n",
    "# pd.DataFrame(dfs_q['Continuous']).to_csv('/all_q_c_perm_filtered.csv', index=False)\n",
    "# pd.DataFrame(dfs_q['Sampled']).to_csv('/all_q_e_perm_filtered.csv', index=False)\n",
    "\n",
    "# # CALL FILTERED Q\n",
    "# fast = pd.read_csv('/all_q_a_perm_filtered.csv', low_memory=False)\n",
    "# science = pd.read_csv('/all_q_b_perm_filtered.csv', low_memory=False)\n",
    "# continuous = pd.read_csv('/all_q_c_perm_filtered.csv', low_memory=False)\n",
    "# sampled = pd.read_csv('/all_q_e_perm_filtered.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a7f899-e386-4a76-85bf-2bb68f7ea1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to run all analyses across runs\n",
    "# Input for almost all functions\n",
    "\n",
    "df_list = [fast, science, continuous] #, runE_perm_clean] #, runD_perm_clean] \n",
    "df_list_common = subset_to_common_reach_ids(df_list)\n",
    "print(df_list_common[0].reach_id.nunique())\n",
    "\n",
    "\n",
    "dfs_q = {'Fast': df_list_common[0], 'Science': df_list_common[1], 'Continuous': df_list_common[2], 'Sampled': sampled}\n",
    "\n",
    "for label, df in dfs_q.items():\n",
    "    df['algo'] = df['algo'].replace({'geobam': 'neobam'})\n",
    "    print(label, df.reach_id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee71d2-3ea1-460c-9409-36b11edcf7e8",
   "metadata": {},
   "source": [
    "## Hist of FS Orbit Q Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dab7db5-ecd3-402e-933c-adfe60a999b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filt = dfs_q['Continuous'].copy()\n",
    "\n",
    "# Ensure 'time' is datetime\n",
    "df_filt['time'] = pd.to_datetime(df_filt['time'])\n",
    "df_filt = df_filt[df_filt['algo'] == 'consensus']\n",
    "df_filt = df_filt.dropna(subset=['Q'])\n",
    "\n",
    "cutoff_date = pd.Timestamp('2023-07-11')\n",
    "\n",
    "# Extract month and year\n",
    "df_filt['month'] = df_filt['time'].dt.month\n",
    "df_filt['year'] = df_filt['time'].dt.year\n",
    "\n",
    "# Label whether a record is FS Orbit (before cutoff) or SO years\n",
    "def assign_group(row):\n",
    "    if row['time'] < cutoff_date:\n",
    "        return 'FS_2023'\n",
    "    elif row['year'] == 2023:\n",
    "        return 'SO_2023'\n",
    "    elif row['year'] == 2024:\n",
    "        return 'SO_2024'\n",
    "    elif row['year'] == 2025:\n",
    "        return 'SO_2025'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df_filt['group'] = df_filt.apply(assign_group, axis=1)\n",
    "df_filt = df_filt.dropna(subset=['group'])\n",
    "print(df_filt.groupby('group')['reach_id'].nunique())\n",
    "print(df_filt[df_filt['group'] == 'FS_2023'].shape[0] / df_filt.shape[0])\n",
    "# Count Q per reach, per month, per group\n",
    "reach_monthly_counts = (\n",
    "    df_filt.groupby(['reach_id', 'month', 'group'])['Q']\n",
    "    .count()\n",
    "    .reset_index(name='Q_count')\n",
    ")\n",
    "\n",
    "# Average across reaches for each (month, group)\n",
    "avg_q_per_reach = (\n",
    "    reach_monthly_counts\n",
    "    .groupby(['month', 'group'])['Q_count']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Pivot to wide format for grouped bar chart\n",
    "pivot_df = avg_q_per_reach.pivot(index='month', columns='group', values='Q_count').fillna(0)\n",
    "\n",
    "# Order columns (bars per month)\n",
    "order = ['FS_2023', 'SO_2023', 'SO_2024', 'SO_2025']\n",
    "pivot_df = pivot_df.reindex(columns=order, fill_value=0)\n",
    "\n",
    "# Define custom colors (lightest to darkest blue)\n",
    "colors = ['black',\n",
    "          'dimgrey',  # light blue (FS Orbit)\n",
    "          'grey',  # steel blue (2023_SO)\n",
    "          'silver',  # dark blue (2024_SO)\n",
    "        ]  # very dark navy (2025_SO)\n",
    "\n",
    "# Plot grouped bar chart\n",
    "ax = pivot_df.plot(kind='bar', figsize=(12, 8), width=0.8, color=colors, edgecolor='black')\n",
    "\n",
    "plt.xlabel('Month', fontsize=30)\n",
    "plt.ylabel('Average Q Instances per Reach', fontsize=30)\n",
    "plt.title('Average Monthly Q per Reach by Year', fontsize=32)\n",
    "plt.yticks(fontsize=26)\n",
    "plt.xticks(\n",
    "    ticks=range(0, 12),\n",
    "    labels=['Jan','Feb','Mar','Apr','May','Jun',\n",
    "            'Jul','Aug','Sep','Oct','Nov','Dec'],\n",
    "    rotation=0,\n",
    "    fontsize=26\n",
    ")\n",
    "plt.legend(title='Time Period', fontsize=26, title_fontsize=28)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/figs/month_freq_poster.png', bbox_inches='tight', dpi=350)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0963fb2-f2bc-4213-91ab-2ced13d7f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post filtering reach ids\n",
    "\n",
    "# # Count how many algos each reach_id has\n",
    "# algo_counts_per_reach = dfs_q['Continuous'].groupby('reach_id')['algo'].nunique()\n",
    "\n",
    "# Count frequency per algorithm\n",
    "df_bar = dfs_q['Continuous']\n",
    "df_bar['algo'] = df_bar['algo'].replace({'geobam': 'neobam'})\n",
    "algo_counts = df_bar.groupby('algo')['reach_id'].nunique()\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "bars = plt.bar(algo_counts.index, algo_counts.values, color=[color_dict[a] for a in algo_counts.index])\n",
    "\n",
    "# Add counts on top of bars\n",
    "for bar in bars:\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2, \n",
    "        bar.get_height(), \n",
    "        str(int(bar.get_height())), \n",
    "        ha='center', va='bottom'\n",
    "    )\n",
    "\n",
    "plt.ylim([0, 11500])\n",
    "plt.ylabel('Reaches', fontsize=28)\n",
    "plt.xlabel('Algorithm', fontsize=28)\n",
    "plt.title('Algorithm Frequency (Continuous run)', fontsize=30)\n",
    "plt.xticks(rotation=30, fontsize=26)\n",
    "plt.yticks(fontsize=26)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/figs/algoFreq_barChart.png', dpi=350)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a4438-9846-427c-a7c8-7924b94526c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############\n",
    "# REVISIT TIME\n",
    "###############\n",
    "\n",
    "color_dict_timeseries = {\n",
    "    'Continuous': '#D83A34',\n",
    "    'Fast': '#FD8500',\n",
    "    'Science': '#2B9EB3',\n",
    "    'Sampled': '#6610F2',\n",
    "    'gauge': 'darkgrey',\n",
    "    'Continuous-FSO': '#FFEE32',\n",
    "    'Continuous-SO': 'turquoise',\n",
    "}\n",
    "\n",
    "#GOOD TO GET HYDROCRON DATA FOR YOUR REACHES HERE\n",
    "fsOrbit = pd.read_csv('/reachHydrocronData.csv')\n",
    "print(fsOrbit.columns.values.tolist())\n",
    "dfs_q_revisit = {}\n",
    "for label, df in dfs_q.items():\n",
    "    df_geo = df.merge(fsOrbit.drop(columns=['time', 'time_str']), how = 'left', on = ['reach_id'])\n",
    "    dfs_q_revisit[label] = df_geo \n",
    "\n",
    "get_revisit_times(dfs_q=dfs_q_revisit, color_dict_timeseries=color_dict_timeseries)\n",
    "get_revisit_times_overall(dfs_q=dfs_q_revisit, color_dict_timeseries=color_dict_timeseries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce5b6f-5a3f-4401-8821-0cd3bc670208",
   "metadata": {},
   "source": [
    "## Plot the Orbit Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd286373-b46f-4790-b374-8fcf8b9d34ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = ['Fast', 'Science', 'Continuous', 'Sampled', 'gauge']\n",
    "\n",
    "color_dict_timeseries = {\n",
    "    'Continuous': '#D83A34',\n",
    "    'Fast': '#FD8500',\n",
    "    'Science': '#2B9EB3',\n",
    "    'Sampled': '#6610F2',\n",
    "    'gauge': 'black'\n",
    "}\n",
    "\n",
    "plot_consensus_from_multiple_dfs(\n",
    "    dfs_dict=dfs_q,\n",
    "    labels=labels,\n",
    "    divide_date=pd.to_datetime('2023-07-11'),\n",
    "    color_dict=color_dict_timeseries,\n",
    "    algo='consensus'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b47d28-4fbb-4a89-a2a4-238231cbb159",
   "metadata": {},
   "source": [
    "## Q Difference Boxplots with Significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b461d78-7adb-4a4e-8467-2e39aa46eea2",
   "metadata": {},
   "source": [
    "### By Continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708dee91-06ce-4e2a-a66d-0f0ea82785f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by reach\n",
    "plot_seasonal_log_ratio_boxplots_by_continent_reach(dfs_dict=dfs_q, consensus_algo='consensus', plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572cd42e-abd3-4b79-832c-4c85ebd66a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_gaugeFullRange = pd.read_csv('/dfs_gauge_fullRange_a_perm_filtered.csv')\n",
    "science_gaugeFullRange = pd.read_csv('/dfs_gauge_fullRange_b_perm_filtered.csv')\n",
    "continuous_gaugeFullRange = pd.read_csv('/dfs_gauge_fullRange_c_perm_filtered.csv')\n",
    "sampled_gaugeFullRange= pd.read_csv('/dfs_gauge_fullRange_e_perm_filtered.csv')\n",
    "\n",
    "dfs_gaugeFullRange = {'Fast': fast_gaugeFullRange, 'Science': science_gaugeFullRange, 'Continuous': continuous_gaugeFullRange, 'Sampled': sampled_gaugeFullRange}\n",
    "\n",
    "# Add continuous orbit split\n",
    "runC_perm_fast_gaugeFullRange = dfs_gaugeFullRange['Continuous'][pd.to_datetime(dfs_gaugeFullRange['Continuous']['time']) < divide_date]\n",
    "runC_perm_science_gaugeFullRange = dfs_gaugeFullRange['Continuous'][pd.to_datetime(dfs_gaugeFullRange['Continuous']['time']) >= divide_date]\n",
    "\n",
    "dfs_gaugeFullRange['Continuous-FSO'] = runC_perm_fast_gaugeFullRange\n",
    "dfs_gaugeFullRange['Continuous-SO'] = runC_perm_science_gaugeFullRange\n",
    "\n",
    "print(dfs_gaugeFullRange['Fast'].algo.unique())\n",
    "\n",
    "    \n",
    "# Add continuous orbit split\n",
    "runC_perm_fast = dfs_q['Continuous'][pd.to_datetime(dfs_q['Continuous']['time']) < divide_date]\n",
    "runC_perm_science = dfs_q['Continuous'][pd.to_datetime(dfs_q['Continuous']['time']) >= divide_date]\n",
    "\n",
    "dfs_q['Continuous-FSO'] = runC_perm_fast\n",
    "dfs_q['Continuous-SO'] = runC_perm_science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b4f30-76aa-40a9-bd70-fc64dcdd0155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare gauged and ungauged discharge\n",
    "\n",
    "plot_seasonal_log_ratio_by_continent_reach_gauged_vs_ungauged(\n",
    "    dfs_dict_gauged=dfs_gaugeFullRange,\n",
    "    dfs_dict_ungauged=dfs_q,\n",
    "    consensus_algo='consensus',\n",
    "    output_dir='/figs/'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50219ca-e6e9-4b14-9c5f-66b91118c606",
   "metadata": {},
   "source": [
    "### Regime Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f778a5-a9b7-43c8-862b-d996bd7b1b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the function\n",
    "aggregated_q_overall = summarize_overall_Q(dfs_q=dfs_q, algo='consensus')\n",
    "\n",
    "# Combine and save\n",
    "overall_summary_all_cons = pd.concat(aggregated_q_overall.values(), ignore_index=True)\n",
    "overall_summary_all_cons.to_csv('/consensus_regime_summary.csv', index=False)\n",
    "print('save successful')\n",
    "\n",
    "\n",
    "# aggregated_q_overall = summarize_overall_Q(dfs_q=dfs_gauge_fullRange, algo='gauge')\n",
    "\n",
    "# # Combine and save\n",
    "# aggregated_q_overall_gauge_fullRange = pd.concat(aggregated_q_overall.values(), ignore_index=True)\n",
    "# aggregated_q_overall_gauge_fullRange.to_csv('/gauge_gauges_regime_summary.csv', index=False)\n",
    "# print('save successful')\n",
    "\n",
    "# aggregated_q_overall = summarize_overall_Q(dfs_q=dfs_gauge_fullRange, algo='gauge_swot_match')\n",
    "\n",
    "# # Combine and save\n",
    "# overall_summary_all_gauge_swot = pd.concat(aggregated_q_overall.values(), ignore_index=True)\n",
    "# overall_summary_all_gauge_swot.to_csv('/gauge_swot_match_gauges_regime_summary.csv', index=False)\n",
    "# print('save successful')\n",
    "\n",
    "\n",
    "aggregated_q_overall = summarize_overall_Q(dfs_q=dfs_q, algo='metroman')\n",
    "\n",
    "# Combine and save\n",
    "overall_summary_all_metroman = pd.concat(aggregated_q_overall.values(), ignore_index=True)\n",
    "overall_summary_all_metroman.to_csv('/metroman_regime_summary.csv', index=False)\n",
    "print('save successful')\n",
    "\n",
    "aggregated_q_overall = summarize_overall_Q(dfs_q=dfs_q, algo='sic4dvar')\n",
    "\n",
    "# Combine and save\n",
    "overall_summary_all_sic4dvar = pd.concat(aggregated_q_overall.values(), ignore_index=True)\n",
    "overall_summary_all_sic4dvar.to_csv('/sic4dvar_regime_summary.csv', index=False)\n",
    "print('save successful')\n",
    "\n",
    "\n",
    "aggregated_q_overall = summarize_overall_Q(dfs_q=dfs_q, algo='momma')\n",
    "\n",
    "# Combine and save\n",
    "overall_summary_all_momma = pd.concat(aggregated_q_overall.values(), ignore_index=True)\n",
    "overall_summary_all_momma.to_csv('/momma_regime_summary.csv', index=False)\n",
    "print('save successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119424b7-5697-4ede-bbc2-11451b20a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_summary_all_cons = pd.read_csv('/consensus_regime_summary.csv')\n",
    "overall_summary_all_metroman = pd.read_csv('/metroman_regime_summary.csv')\n",
    "overall_summary_all_sic4dvar = pd.read_csv('/sic4dvar_regime_summary.csv')\n",
    "overall_summary_all_momma = pd.read_csv('/momma_regime_summary.csv')\n",
    "overall_summary_all_neobam = pd.read_csv('/neobam_regime_summary.csv')\n",
    "\n",
    "overall_summary = pd.concat([overall_summary_all_cons,overall_summary_all_metroman,overall_summary_all_sic4dvar,overall_summary_all_momma])\n",
    "print(overall_summary.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476b084-b079-4517-8051-331a3b02ab4f",
   "metadata": {},
   "source": [
    "#### Overall Algo by orbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f6468-1a7e-4d44-8cfb-790c306c9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "color_dict = {\n",
    "    'sic4dvar': 'green',\n",
    "    'momma': 'blue',\n",
    "    'neobam': 'purple',\n",
    "    'consensus': 'sienna',\n",
    "    'metroman': 'orange',\n",
    "    'geobam': 'purple',\n",
    "    'hivdi': 'deeppink',\n",
    "    'sad': 'tomato',\n",
    "    'gauge': 'black'\n",
    "}\n",
    "\n",
    "algos = ['momma', 'sic4dvar', 'metroman', 'neobam', 'geobam', 'consensus']\n",
    "orbit_pairs = [\n",
    "    ('Science', 'Continuous'),\n",
    "    ('Fast', 'Continuous'),\n",
    "    ('Fast', 'Sampled'),\n",
    "]\n",
    "\n",
    "plot_logQ_diff_grouped_by_orbit_combo_with_stats(\n",
    "    dfs_dict=dfs_q,\n",
    "    algos=algos,\n",
    "    orbit_pairs=orbit_pairs,\n",
    "    color_dict=color_dict\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ca3e8a-1bb8-4a0b-a0d1-72ed02867e3b",
   "metadata": {},
   "source": [
    "## Reach Comparison - Q Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b694277-5ff4-46a9-916b-1bd5dc2deeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import ks_2samp, wasserstein_distance\n",
    "\n",
    "\n",
    "\n",
    "color_dict_runCombos = {\n",
    "    'Continuous': '#D83A34',\n",
    "    'Fast': '#FD8500',\n",
    "    'Science': '#2B9EB3',\n",
    "    'Sampled': '#6610F2',\n",
    "    'gauge': 'black',\n",
    "    'Continuous-FSO': '#FFEE32',\n",
    "    'Continuous-SO': 'turquoise',\n",
    "}\n",
    "\n",
    "# Add continuous orbit split\n",
    "runC_perm_fast = dfs_q['Continuous'][pd.to_datetime(dfs_q['Continuous']['time']) < divide_date]\n",
    "runC_perm_science = dfs_q['Continuous'][pd.to_datetime(dfs_q['Continuous']['time']) >= divide_date]\n",
    "\n",
    "dfs_q['Continuous-FSO'] = runC_perm_fast\n",
    "dfs_q['Continuous-SO'] = runC_perm_science\n",
    "\n",
    "\n",
    "summary_metrics = plot_reach_consensus_cdfs(\n",
    "    df_dict=dfs_q,\n",
    "    variable='Q',\n",
    "    algo='consensus',\n",
    "    color_dict=color_dict_runCombos\n",
    ")\n",
    "\n",
    "summary_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fdc22c-a760-421c-afca-009f2373e676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes awhile\n",
    "\n",
    "color_dict_runCombos = {\n",
    "    'Continuous': '#D83A34',\n",
    "    'Fast': '#FD8500',\n",
    "    'Science': '#2B9EB3',\n",
    "    'Sampled': '#6610F2',\n",
    "    'gauge': 'black',\n",
    "    'Continuous-FSO': '#FFEE32',\n",
    "    'Continuous-SO': 'blue',\n",
    "}\n",
    "\n",
    "# Add continuous orbit split\n",
    "runC_perm_fast = dfs_q['Continuous'][pd.to_datetime(dfs_q['Continuous']['time']) < divide_date]\n",
    "runC_perm_science = dfs_q['Continuous'][pd.to_datetime(dfs_q['Continuous']['time']) >= divide_date]\n",
    "\n",
    "dfs_q['Continuous-FSO'] = runC_perm_fast\n",
    "dfs_q['Continuous-SO'] = runC_perm_science\n",
    "\n",
    "# # \n",
    "summary_metrics = plot_reach_consensus_cdfs(\n",
    "    df_dict=dfs_q,\n",
    "    variable='Q',\n",
    "    algo='consensus',\n",
    "    color_dict=color_dict_runCombos\n",
    ")\n",
    "\n",
    "summary_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de37b6-1427-462c-a9b7-e55084394819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join to FS reaches\n",
    "fsOrbit = pd.read_csv('/figs/fsOrbit_hydrocron.csv')\n",
    "\n",
    "summary_metrics_geo = summary_metrics.merge(fsOrbit, how = 'left', on = ['reach_id'])\n",
    "\n",
    "print(summary_metrics_geo.shape)\n",
    "print(summary_metrics_geo)\n",
    "summary_metrics_geo.to_csv('/figs/summary_reach_metrics_orbit_geo.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a8412d-2fa5-4fac-a5b0-c9852254f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_metrics_geo = pd.read_csv('/figs/summary_reach_metrics_orbit_geo.csv')\n",
    "print(summary_metrics_geo.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fc6134-50fd-4100-b97e-e81ea621b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary metric plots\n",
    "\n",
    "plot_pair_stacked_bars(df=summary_metrics_geo, metric='nBIAS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926df893-530d-4473-8c32-5e5bb79f808c",
   "metadata": {},
   "source": [
    "### by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed509b93-04cd-477c-9ee6-7b7ab8cf2134",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict_runCombos = {\n",
    "    'Continuous': '#D83A34', #red\n",
    "    'Fast': '#FD8500', #Orange\n",
    "    'Science': '#2B9EB3', #blue\n",
    "    'Sampled': '#6610F2', #purple\n",
    "    'gauge': 'black',\n",
    "    'Continuous-FSO': '#FFEE32', #yellow\n",
    "    'Continuous-SO': 'blue', #blue\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddfdc58-4d98-46be-ba13-f6b568b604b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pair_by_continent(df=summary_metrics_geo, pairs_to_plot=[\"Fast-Continuous-FSO\", \"Science-Continuous-SO\"], metric='nBIAS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6938f99-6810-443d-8e06-c0eecbafaac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-swotEF]",
   "language": "python",
   "name": "conda-env-.conda-swotEF-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
